{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13953250,"sourceType":"datasetVersion","datasetId":8872686},{"sourceId":13979979,"sourceType":"datasetVersion","datasetId":8911827}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:10.584946Z","iopub.execute_input":"2025-12-03T22:21:10.585625Z","iopub.status.idle":"2025-12-03T22:21:10.594503Z","shell.execute_reply.started":"2025-12-03T22:21:10.585601Z","shell.execute_reply":"2025-12-03T22:21:10.593703Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/freedoom2-wad/freedoom2.wad\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/vizdoom')  # donde quede el dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:10.595763Z","iopub.execute_input":"2025-12-03T22:21:10.596024Z","iopub.status.idle":"2025-12-03T22:21:10.612715Z","shell.execute_reply.started":"2025-12-03T22:21:10.596004Z","shell.execute_reply":"2025-12-03T22:21:10.612060Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"pip install vizdoom --pre","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:10.613440Z","iopub.execute_input":"2025-12-03T22:21:10.613704Z","iopub.status.idle":"2025-12-03T22:21:14.228916Z","shell.execute_reply.started":"2025-12-03T22:21:10.613682Z","shell.execute_reply":"2025-12-03T22:21:14.227950Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: vizdoom in /usr/local/lib/python3.11/dist-packages (1.3.0.dev3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vizdoom) (1.26.4)\nRequirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from vizdoom) (0.29.0)\nRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from vizdoom) (2.6.1)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->vizdoom) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->vizdoom) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vizdoom) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vizdoom) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vizdoom) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->vizdoom) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->vizdoom) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->vizdoom) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# CELDA 1: Imports (sin cambios)\nimport numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport sys\nsys.path.append('/kaggle/input/vizdoom')\n\n!pip install vizdoom --pre\n\nfrom vizdoom import *\nimport cv2\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport imageio\nimport seaborn as sns\nfrom IPython.display import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:14.230748Z","iopub.execute_input":"2025-12-03T22:21:14.231494Z","iopub.status.idle":"2025-12-03T22:21:17.707511Z","shell.execute_reply.started":"2025-12-03T22:21:14.231467Z","shell.execute_reply":"2025-12-03T22:21:17.706719Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/freedoom2-wad/freedoom2.wad\nRequirement already satisfied: vizdoom in /usr/local/lib/python3.11/dist-packages (1.3.0.dev3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vizdoom) (1.26.4)\nRequirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from vizdoom) (0.29.0)\nRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from vizdoom) (2.6.1)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->vizdoom) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->vizdoom) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vizdoom) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vizdoom) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vizdoom) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->vizdoom) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->vizdoom) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->vizdoom) (2024.2.0)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# CELDA 2: DoomFreedoomEnv â€” VERSIÃ“N \"NO COWARDS\" (Anti-Retroceso)\nimport numpy as np\nimport cv2\nfrom vizdoom import *\nfrom gym import spaces\n\nclass DoomFreedoomEnv:\n    def __init__(self):\n        self.game = DoomGame()\n        self.game.set_doom_game_path(\"/kaggle/input/freedoom2-wad\")\n        self.game.set_doom_scenario_path(\"/kaggle/input/freedoom2-wad\")\n        self.game.set_doom_map(\"MAP01\")\n\n        self.game.set_window_visible(False)\n        self.game.set_sound_enabled(False)\n        self.game.set_screen_format(ScreenFormat.GRAY8)\n        self.game.set_screen_resolution(ScreenResolution.RES_320X240)\n\n        # Acciones: 0:FWD, 1:BACK, 2:LEFT, 3:RIGHT, 4:ATTACK\n        self.actions = [\n            [1, 0, 0, 0, 0], \n            [0, 1, 0, 0, 0], \n            [0, 0, 1, 0, 0], \n            [0, 0, 0, 1, 0], \n            [0, 0, 0, 0, 1], \n        ]\n        \n        self.buttons = [Button.MOVE_FORWARD, Button.MOVE_BACKWARD, Button.TURN_LEFT, Button.TURN_RIGHT, Button.ATTACK]\n        for b in self.buttons: self.game.add_available_button(b)\n\n        for v in [GameVariable.POSITION_X, GameVariable.POSITION_Y, GameVariable.ANGLE, GameVariable.KILLCOUNT, GameVariable.HEALTH]:\n            self.game.add_available_game_variable(v)\n\n        # Living reward negativo muy pequeÃ±o para forzar movimiento constante\n        self.game.set_living_reward(-0.0001) \n        \n        # 8400 ticks = tiempo suficiente para cruzar el mapa\n        self.game.set_episode_timeout(8400) \n        \n        self.game.set_death_penalty(1.0)\n        self.game.set_mode(Mode.PLAYER)\n        self.game.init()\n\n        self.prev_x = 0; self.prev_y = 0; self.prev_kills = 0; self.prev_health = 0\n        self.action_space_size = len(self.actions)\n\n    def _get_frame(self):\n        state = self.game.get_state()\n        if state is None: return np.zeros((84, 84), dtype=np.uint8)\n        return cv2.resize(state.screen_buffer, (84, 84)).astype(np.uint8)\n\n    def reset(self):\n        self.game.new_episode()\n        state = self.game.get_state()\n        v = state.game_variables\n        self.prev_x, self.prev_y = v[0], v[1]\n        self.prev_kills = v[3]; self.prev_health = v[4]\n        return self._get_frame()\n\n    def step(self, action_id):\n        # action_id 0 = FWD, 1 = BACK\n        reward_base = self.game.make_action(self.actions[action_id], 4)\n        done = self.game.is_episode_finished()\n        state = self.game.get_state()\n        \n        if state is None or state.game_variables[4] <= 0:\n            return np.zeros((84, 84), dtype=np.uint8), -1.0, True\n\n        x, y, angle, kills, health = state.game_variables\n        reward = 0.0\n        \n        # --- REWARD SHAPING CORREGIDO ---\n        \n        # 1. Kills: Lo mÃ¡s importante\n        if kills > self.prev_kills: \n            reward += 1.0 \n        \n        # 2. Movimiento inteligente\n        # Penalizar marcha atrÃ¡s (AcciÃ³n 1)\n        if action_id == 1:\n            reward -= 0.01  # PequeÃ±o castigo por retroceder (solo Ãºsalo si es urgente)\n        \n        # Incentivar ir hacia adelante (AcciÃ³n 0) si realmente se mueve\n        dist = np.sqrt((x - self.prev_x)**2 + (y - self.prev_y)**2)\n        if action_id == 0 and dist > 1.0:\n            reward += 0.002 # PequeÃ±o premio por avanzar\n            \n        # 3. Penalizar daÃ±o\n        if health < self.prev_health: \n            reward -= 0.05\n\n        self.prev_x, self.prev_y = x, y\n        self.prev_kills = kills\n        self.prev_health = health\n\n        return self._get_frame(), np.clip(reward, -1.0, 1.0), done","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:17.708525Z","iopub.execute_input":"2025-12-03T22:21:17.708804Z","iopub.status.idle":"2025-12-03T22:21:17.722611Z","shell.execute_reply.started":"2025-12-03T22:21:17.708781Z","shell.execute_reply":"2025-12-03T22:21:17.721850Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# CELDA 3: Instancia y plot (sin cambios)\nenv = DoomFreedoomEnv()\nframe = env.reset()\nprint(\"Raw frame shape:\", frame.shape)\nplt.imshow(frame, cmap='gray')\nplt.title(\"Freedoom MAP01 â€” Frame inicial\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:17.723469Z","iopub.execute_input":"2025-12-03T22:21:17.723678Z","iopub.status.idle":"2025-12-03T22:21:17.766903Z","shell.execute_reply.started":"2025-12-03T22:21:17.723663Z","shell.execute_reply":"2025-12-03T22:21:17.765234Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileDoesNotExistException\u001b[0m                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1755194036.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# CELDA 3: Instancia y plot (sin cambios)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoomFreedoomEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Raw frame shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2067778526.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_death_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPLAYER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_kills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_health\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileDoesNotExistException\u001b[0m: File \"/kaggle/input/freedoom2-wad(.wad)\" does not exist."],"ename":"FileDoesNotExistException","evalue":"File \"/kaggle/input/freedoom2-wad(.wad)\" does not exist.","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"# CELDA 4: FrameStack (sin cambios)\nclass FrameStack:\n    def __init__(self, k=4):\n        self.k = k\n        self.frames = deque([], maxlen=k)\n\n    def reset(self, frame):\n        self.frames = deque([frame]*self.k, maxlen=self.k)\n        return np.stack(self.frames, axis=0)\n\n    def step(self, frame):\n        self.frames.append(frame)\n        return np.stack(self.frames, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:17.767541Z","iopub.status.idle":"2025-12-03T22:21:17.767934Z","shell.execute_reply.started":"2025-12-03T22:21:17.767719Z","shell.execute_reply":"2025-12-03T22:21:17.767737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELDA 5: ARQUITECTURA RECURRENTE (CNN + GRU)\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef init_layer(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n\nclass RecurrentActorCritic(nn.Module):\n    def __init__(self, input_shape, action_dim):\n        super().__init__()\n        \n        # 1. Feature Extractor (CNN) - Igual que antes\n        self.conv = nn.Sequential(\n            init_layer(nn.Conv2d(4, 32, 8, stride=4)), nn.ReLU(),\n            init_layer(nn.Conv2d(32, 64, 4, stride=2)), nn.ReLU(),\n            init_layer(nn.Conv2d(64, 64, 3, stride=1)), nn.ReLU()\n        )\n\n        with torch.no_grad():\n            dummy = torch.zeros(1, *input_shape)\n            out = self.conv(dummy)\n            self.flat_size = out.view(1, -1).shape[1]\n            print(f\"âœ” CNN Output Size: {self.flat_size}\")\n\n        # 2. MEMORIA RECURRENTE (GRU)\n        # Recibe los features de la CNN y la memoria anterior\n        self.gru_hidden_size = 512\n        self.gru = nn.GRU(self.flat_size, self.gru_hidden_size, batch_first=True)\n        \n        # 3. Cabezas Actor y Critic\n        self.actor = init_layer(nn.Linear(self.gru_hidden_size, action_dim), std=0.01)\n        self.critic = init_layer(nn.Linear(self.gru_hidden_size, 1), std=1.0)\n        \n        # Red auxiliar para RND (Feature Extractor base)\n        # Necesitamos una forma de extraer features sin pasar por la GRU para el RND\n        self.fc_rnd = nn.Linear(self.flat_size, 512)\n\n    def forward(self, x, hidden=None):\n        # x shape: (Batch, 4, 84, 84)\n        \n        # NormalizaciÃ³n\n        if x.max() > 1.0: x = x.float() / 255.0\n            \n        # CNN\n        features = self.conv(x)\n        features = features.view(features.size(0), -1) # Flatten (Batch, flat_size)\n        \n        # GRU Logic\n        # La GRU espera (Batch, Seq_Len, Features). AquÃ­ Seq_Len=1 porque vamos paso a paso.\n        features_seq = features.unsqueeze(1) \n        \n        if hidden is None:\n            # Si es el primer paso, memoria en ceros\n            hidden = torch.zeros(1, x.size(0), self.gru_hidden_size).to(device)\n            \n        # Forward GRU\n        gru_out, new_hidden = self.gru(features_seq, hidden)\n        \n        # Quitamos la dimensiÃ³n de secuencia para las capas finales\n        gru_out_flat = gru_out[:, -1, :]\n        \n        return self.actor(gru_out_flat), self.critic(gru_out_flat), new_hidden\n\n    def get_features_for_rnd(self, x):\n        # FunciÃ³n auxiliar solo para RND (ignora la memoria temporal)\n        if x.max() > 1.0: x = x.float() / 255.0\n        f = self.conv(x)\n        f = f.view(f.size(0), -1)\n        return torch.relu(self.fc_rnd(f))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:17.769535Z","iopub.status.idle":"2025-12-03T22:21:17.769918Z","shell.execute_reply.started":"2025-12-03T22:21:17.769704Z","shell.execute_reply":"2025-12-03T22:21:17.769720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELDA 6: RND OPTIMIZADO (Estable y Determinista)\nclass RND(nn.Module):\n    def __init__(self, feat_dim=512):\n        super().__init__()\n        \n        # Redujimos un poco la dimensiÃ³n oculta para forzar compresiÃ³n de informaciÃ³n\n        dim_h = 128 \n        \n        # TARGET: Red aleatoria fija. \n        # Â¡CRÃTICO: SIN DROPOUT! El target debe ser determinista.\n        # Usamos init_layer (definida en Celda 5) para asegurar buena distribuciÃ³n inicial.\n        self.target = nn.Sequential(\n            init_layer(nn.Linear(feat_dim, dim_h)), \n            nn.LeakyReLU(), # LeakyReLU es mejor para evitar neuronas muertas en el target\n            init_layer(nn.Linear(dim_h, dim_h))\n        )\n        \n        # PREDICTOR: Intenta imitar al target.\n        # Hacemos el predictor un poco mÃ¡s profundo para que tenga capacidad de aprender\n        self.predictor = nn.Sequential(\n            init_layer(nn.Linear(feat_dim, dim_h)), \n            nn.LeakyReLU(),\n            init_layer(nn.Linear(dim_h, dim_h)), \n            nn.LeakyReLU(),\n            init_layer(nn.Linear(dim_h, dim_h))\n        )\n        \n        # Congelamos el Target (nunca se entrena)\n        for p in self.target.parameters():\n            p.requires_grad = False\n\n    def forward(self, feat):\n        # IMPORTANTE: feat viene de la CNN principal.\n        # Debemos usar .detach() para que el entrenamiento del RND \n        # NO modifique los pesos de la CNN (Feature Extractor).\n        # La CNN debe aprender a ver enemigos, no a facilitar la vida del RND.\n        \n        feat_in = feat.detach() \n        \n        t = self.target(feat_in)\n        p = self.predictor(feat_in)\n        return p, t","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:17.771256Z","iopub.status.idle":"2025-12-03T22:21:17.771642Z","shell.execute_reply.started":"2025-12-03T22:21:17.771414Z","shell.execute_reply":"2025-12-03T22:21:17.771432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELDA 7: PPO UPDATE (Soporte Recurrente)\ndef ppo_update(model, rnd, optM, optR, data, clip=0.2, epoch=4, batch=64):\n    # Ahora recibimos tambiÃ©n 'hiddens' en la data\n    obs, acts, old_logp, returns, adv, hiddens = data\n    \n    obs = torch.tensor(obs, dtype=torch.float32).to(device)\n    acts = torch.tensor(acts, dtype=torch.long).to(device)\n    old_logp = torch.tensor(old_logp, dtype=torch.float32).to(device)\n    returns = torch.tensor(returns, dtype=torch.float32).to(device)\n    adv = torch.tensor(adv, dtype=torch.float32).to(device)\n    \n    # El hidden state almacenado (Batch, 1, 512) -> Transponer a (1, Batch, 512) para GRU\n    hiddens = torch.tensor(hiddens, dtype=torch.float32).to(device).permute(1, 0, 2).contiguous()\n\n    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n    dataset_size = len(obs)\n    total_loss_val = 0\n    \n    for _ in range(epoch):\n        idxs = np.random.permutation(dataset_size)\n        \n        for start in range(0, dataset_size, batch):\n            b = idxs[start:start + batch]\n            \n            # Recortamos el hidden state correspondiente a este batch\n            # Shape necesaria: (1, Batch_Size, 512)\n            batch_hidden = hiddens[:, b, :] \n            \n            # --- FORWARD CON MEMORIA ---\n            # Pasamos el hidden state guardado. \n            # Nota: Esto entrena el \"paso actual\" condicionando al recuerdo anterior.\n            logits, values, _ = model(obs[b], batch_hidden)\n            values = values.squeeze()\n\n            if torch.isnan(logits).any(): continue\n\n            dist = torch.distributions.Categorical(logits=logits)\n            new_logp = dist.log_prob(acts[b])\n            entropy = dist.entropy().mean()\n\n            ratio = torch.exp(new_logp - old_logp[b])\n            surr1 = ratio * adv[b]\n            surr2 = torch.clamp(ratio, 1.0 - clip, 1.0 + clip) * adv[b]\n            actor_loss = -torch.min(surr1, surr2).mean()\n            \n            critic_loss = 0.5 * ((returns[b] - values) ** 2).mean()\n\n            # Mantenemos EntropÃ­a Alta (0.05) como acordamos\n            loss = actor_loss + critic_loss - 0.05 * entropy \n\n            optM.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optM.step()\n            total_loss_val += loss.item()\n\n            # --- RND UPDATE ---\n            feat_rnd = model.get_features_for_rnd(obs[b]) # Usamos la funciÃ³n auxiliar\n            p, t = rnd(feat_rnd.detach())\n            rnd_loss = ((p - t) ** 2).mean()\n            optR.zero_grad()\n            rnd_loss.backward()\n            torch.nn.utils.clip_grad_norm_(rnd.parameters(), 0.5)\n            optR.step()\n\n    return total_loss_val / (epoch * (dataset_size // batch))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:17.772857Z","iopub.status.idle":"2025-12-03T22:21:17.773102Z","shell.execute_reply.started":"2025-12-03T22:21:17.772995Z","shell.execute_reply":"2025-12-03T22:21:17.773005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELDA 8: BUCLE DE ENTRENAMIENTO (CON MEMORIA LSTM/GRU)\nimport torch.optim as optim\nfrom collections import deque\nimport os\n\n# --- CONFIGURACIÃ“N ---\nMAX_EPISODES = 5000      \nBATCH_SIZE = 64          \nLR_ACTOR = 1e-4          \nLR_RND = 1e-4\nGAMMA = 0.99\nGAE_LAMBDA = 0.95\nSAVE_INTERVAL = 100       \n\nenv = DoomFreedoomEnv() # AsegÃºrate de tener la versiÃ³n Celda 2 \"NO COWARDS\" cargada\naction_dim = env.action_space_size\n\n# Instanciamos el modelo RECURRENTE\nagent = RecurrentActorCritic((4, 84, 84), action_dim).to(device)\nrnd_model = RND(feat_dim=512).to(device) # RND usa el output size de la fc_rnd\n\nopt_agent = optim.Adam(agent.parameters(), lr=LR_ACTOR)\nopt_rnd = optim.Adam(rnd_model.parameters(), lr=LR_RND)\nsched_agent = optim.lr_scheduler.StepLR(opt_agent, step_size=1000, gamma=0.9)\n\nprint(f\"ðŸ§  Iniciando Agente Recurrente (GRU) por {MAX_EPISODES} episodios.\")\n\nloss_history, reward_history, kill_history = [], [], []\n\nfor ep in range(MAX_EPISODES):\n    # Buffer ahora incluye 'b_hiddens'\n    b_obs, b_acts, b_logp, b_rews, b_vals, b_int_rews, b_hiddens = [], [], [], [], [], [], []\n    \n    frame = env.reset()\n    state_stack = deque([frame] * 4, maxlen=4)\n    \n    # INICIALIZAR MEMORIA (1, 1, 512)\n    hidden = torch.zeros(1, 1, 512).to(device)\n    \n    done = False\n    ep_reward = 0\n    ep_kills = 0 \n    \n    while not done:\n        state_np = np.array(state_stack)\n        state_tensor = torch.tensor(state_np, dtype=torch.float32).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            # Pasamos el estado y la memoria\n            logits, val, new_hidden = agent(state_tensor, hidden)\n            \n            dist = torch.distributions.Categorical(logits=logits)\n            action = dist.sample()\n            logp = dist.log_prob(action)\n            \n            # RND sobre features planos\n            feat = agent.get_features_for_rnd(state_tensor)\n            p, t = rnd_model(feat)\n            int_reward = ((p - t) ** 2).mean().item()\n\n        next_frame, reward, done = env.step(action.item())\n        \n        if reward >= 0.9: ep_kills += 1\n\n        # Guardamos datos + HIDDEN STATE ACTUAL (Crucial: detach para romper grafo)\n        b_obs.append(state_np)\n        b_acts.append(action.item())\n        b_logp.append(logp.item())\n        b_rews.append(reward)\n        b_int_rews.append(int_reward)\n        b_vals.append(val.item())\n        # Guardamos la memoria que generÃ³ esta acciÃ³n (cpu numpy para ahorrar VRAM)\n        b_hiddens.append(hidden.cpu().numpy().squeeze(0)) \n\n        # Actualizamos para el siguiente paso\n        hidden = new_hidden\n        ep_reward += reward\n        state_stack.append(next_frame)\n        \n    # --- UPDATE (Igual que antes pero pasando b_hiddens) ---\n    b_obs = np.array(b_obs)\n    b_acts = np.array(b_acts)\n    b_logp = np.array(b_logp)\n    b_vals = np.array(b_vals)\n    b_rews = np.array(b_rews)\n    b_int_rews = np.array(b_int_rews)\n    b_hiddens = np.array(b_hiddens) # Shape (Steps, 1, 512)\n    \n    # RND Norm\n    if np.std(b_int_rews) > 0:\n        b_int_rews = (b_int_rews - np.mean(b_int_rews)) / np.std(b_int_rews)\n    b_int_rews = np.clip(b_int_rews, -1, 1) * 0.1\n    \n    total_rewards = b_rews + b_int_rews\n    returns = np.zeros_like(total_rewards); adv = np.zeros_like(total_rewards); lastgaelam = 0\n    \n    for t in reversed(range(len(b_rews))):\n        if t == len(b_rews) - 1: nextnonterminal = 0.0; nextvalues = 0.0\n        else: nextnonterminal = 1.0; nextvalues = b_vals[t+1]\n        delta = total_rewards[t] + GAMMA * nextvalues * nextnonterminal - b_vals[t]\n        adv[t] = lastgaelam = delta + GAMMA * GAE_LAMBDA * nextnonterminal * lastgaelam\n    returns = adv + b_vals\n    \n    if len(b_obs) >= BATCH_SIZE:\n        # Pasamos b_hiddens a ppo_update\n        loss = ppo_update(agent, rnd_model, opt_agent, opt_rnd, \n                          (b_obs, b_acts, b_logp, returns, adv, b_hiddens), \n                          batch=BATCH_SIZE)\n        loss_history.append(loss)\n    else: loss = 0\n\n    sched_agent.step()\n    reward_history.append(ep_reward); kill_history.append(ep_kills)\n    \n    if ep % 10 == 0:\n        avg_r = np.mean(reward_history[-10:]) if len(reward_history) > 0 else 0\n        avg_k = np.mean(kill_history[-10:]) if len(kill_history) > 0 else 0\n        print(f\"[EP {ep}] Rew: {ep_reward:.2f} (Avg: {avg_r:.2f}) | Kills: {ep_kills} (Avg: {avg_k:.1f}) | Loss: {loss:.4f}\")\n    \n    if ep > 0 and ep % SAVE_INTERVAL == 0:\n        torch.save(agent.state_dict(), \"doom_agent_gru.pth\")\n        print(f\"ðŸ’¾ Checkpoint GRU guardado en ep {ep}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:17.773780Z","iopub.status.idle":"2025-12-03T22:21:17.774045Z","shell.execute_reply.started":"2025-12-03T22:21:17.773927Z","shell.execute_reply":"2025-12-03T22:21:17.773938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELDA 9: VisualizaciÃ³n de Resultados (VersiÃ³n GRU)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_rewards(rewards, kills=None):\n    plt.figure(figsize=(12, 5))\n    \n    # --- GRÃFICA 1: REWARDS ---\n    plt.subplot(1, 2, 1)\n    plt.plot(rewards, label='Reward Crudo', color='dodgerblue', alpha=0.3)\n    \n    if len(rewards) >= 50:\n        # Media mÃ³vil mÃ¡s larga para ver tendencia en 5000 episodios\n        moving_avg = np.convolve(rewards, np.ones(50)/50, mode='valid')\n        plt.plot(range(49, len(rewards)), moving_avg, color='navy', linewidth=2, label='Tendencia (Media 50 ep)')\n    \n    plt.xlabel(\"Episodio\")\n    plt.ylabel(\"Reward Total\")\n    plt.title(\"Entrenamiento Recurrente (PPO + GRU + RND)\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # --- GRÃFICA 2: KILLS ---\n    if kills is not None and len(kills) > 0:\n        plt.subplot(1, 2, 2)\n        plt.plot(kills, label='Kills', color='limegreen', alpha=0.4)\n        \n        if len(kills) >= 50:\n            avg_kills = np.convolve(kills, np.ones(50)/50, mode='valid')\n            plt.plot(range(49, len(kills)), avg_kills, color='darkgreen', linewidth=2, label='Tendencia')\n            \n        plt.xlabel(\"Episodio\")\n        plt.ylabel(\"Enemigos Eliminados\")\n        plt.title(\"Eficacia de Combate\")\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n# Detectar variables automÃ¡ticamente\nrewards_data = reward_history if 'reward_history' in globals() else []\nkills_data = kill_history if 'kill_history' in globals() else []\n\nif len(rewards_data) > 0:\n    plot_rewards(rewards_data, kills_data)\nelse:\n    print(\"âš  No hay datos para graficar aÃºn.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:17.775393Z","iopub.status.idle":"2025-12-03T22:21:17.775687Z","shell.execute_reply.started":"2025-12-03T22:21:17.775523Z","shell.execute_reply":"2025-12-03T22:21:17.775538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELDA 10: EVALUACIÃ“N PARA AGENTE RECURRENTE (GRU)\nimport imageio\nimport matplotlib.pyplot as plt\nfrom collections import deque\nimport torch.nn.functional as F\n\ndef evaluate_recurrent_agent(env, model, episodes=3, max_steps=2100, stochastic=True, gif_name=\"eval_gru.gif\"):\n    print(f\"ðŸ§  Evaluando Agente con MEMORIA (GRU)...\")\n    model.eval()\n    \n    total_rewards = []\n    frames_for_gif = []\n    \n    for ep in range(episodes):\n        frame = env.reset()\n        state_stack = deque([frame] * 4, maxlen=4)\n        \n        # --- CLAVE GRU: INICIALIZAR MEMORIA ---\n        # Shape: (Num_Layers, Batch_Size, Hidden_Size) -> (1, 1, 512)\n        hidden = torch.zeros(1, 1, 512).to(device)\n        \n        done = False\n        step = 0\n        ep_reward = 0\n        \n        if ep == 0: frames_for_gif.append(frame)\n        print(f\"--- Episodio {ep+1} ---\")\n\n        while not done and step < max_steps:\n            step += 1\n            \n            # Preprocesamiento\n            state_np = np.array(state_stack)\n            x = torch.tensor(state_np, dtype=torch.float32).unsqueeze(0).to(device)\n            \n            with torch.no_grad():\n                # --- CLAVE GRU: PASAR Y RECIBIR MEMORIA ---\n                # Ahora recibimos 3 valores: logits, value, y la NUEVA memoria\n                logits, _, new_hidden = model(x, hidden)\n                \n                # Actualizamos la memoria para el siguiente paso\n                hidden = new_hidden\n\n                if stochastic:\n                    dist = torch.distributions.Categorical(logits=logits)\n                    action = dist.sample().item()\n                else:\n                    action = torch.argmax(logits, dim=-1).item()\n\n            # Environment Step\n            next_frame, reward, done = env.step(action)\n            state_stack.append(next_frame)\n            ep_reward += reward\n            \n            if ep == 0: frames_for_gif.append(next_frame)\n\n        total_rewards.append(ep_reward)\n        print(f\"   Terminado con Reward: {ep_reward:.2f}\")\n\n    # GIF\n    if len(frames_for_gif) > 0:\n        print(f\"ðŸ’¾ Guardando GIF memoria: {gif_name}\")\n        rgb_frames = [np.stack([f]*3, axis=-1) for f in frames_for_gif]\n        imageio.mimsave(gif_name, rgb_frames, fps=25)\n\n    return total_rewards\n\n# EjecuciÃ³n\nif 'agent' in globals():\n    evaluate_recurrent_agent(env, agent, episodes=1, stochastic=True)\nelse:\n    print(\"âš  Define 'agent' primero.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:17.776632Z","iopub.status.idle":"2025-12-03T22:21:17.777010Z","shell.execute_reply.started":"2025-12-03T22:21:17.776804Z","shell.execute_reply":"2025-12-03T22:21:17.776819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELDA 11: random_agent + plot (sin cambios mayores)\ndef random_agent(env, num_eval=100):\n    rewards = []\n    for _ in range(num_eval):\n        frame = env.reset()\n        done = False\n        ep_r = 0\n        while not done:\n            act = np.random.choice(5)\n            _, r, done = env.step(act)\n            ep_r += r\n        rewards.append(ep_r)\n    return rewards\n\ndef make_random_gif(env, gif_name=\"random_agent.gif\"):\n    frame = env.reset()\n    frames = []\n    done = False\n    while not done:\n        act = np.random.choice(5)\n        next_frame, _, done = env.step(act)\n        frames.append(next_frame)\n    rgb_frames = [np.stack([f]*3, -1) for f in frames]\n    imageio.mimsave(gif_name, rgb_frames, fps=20)\n    print(f\"GIF random: {gif_name}\")\n\nrandom_rewards = random_agent(env)\nmake_random_gif(env)\n\nmin_len = min(100, len(rewards))\nplt.figure(figsize=(12,6))\nplt.plot(rewards[-min_len:], label=\"PPO\")\nplt.plot(random_rewards[-min_len:], label=\"Random\")\nplt.title(\"PPO vs Random (Ãºlt 100 eps)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nImage(filename=\"random_agent.gif\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T22:21:17.777952Z","iopub.status.idle":"2025-12-03T22:21:17.778207Z","shell.execute_reply.started":"2025-12-03T22:21:17.778086Z","shell.execute_reply":"2025-12-03T22:21:17.778099Z"}},"outputs":[],"execution_count":null}]}